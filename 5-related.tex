\section{Related Work}
\label{sec:related}
\todo{Add more related work papers}

\textbf{\circnum{1} Programming model extensions for task affinity.}
Several approaches have introduced programming model extensions that allow users to express task data usage, enabling runtimes to make informed scheduling decisions based on data locality.
In~\cite{terbovenApproachesTaskAffinity2016}, the authors explore various affinity clause options for tasks, taskgroups and taskloops. For tasks, data references or thread-ids can be provided; for taskgroups and taskloops the thread distribution policies are provided (e.g.~spread). 
Our system is similar in that we can configure affinity to groups of tasks or singular tasks. 
Moreover, our system provides useful abstractions to represent the affinity in terms of the hardware, instead of threda-ids. Distribution policies can be built on top of them.
In addition, our system allows specifying the scheduling policy for tasks within the same taskgroup, and supports hierarchical nesting of taskgroups.
In subsequent work by the same authors~\cite{klinkenbergAssessingTasktoDataAffinity2018a}, they evaluated the affinity clause using an experimental implementation for OpenMP 5.0 in LLVM \libomp.
The affinity clause provides hints to the runtime about task data usage, enabling the scheduler to select appropriate CPUs, potentially those closest to data location. 
Two main modes of action were tested: (1) Domain mode in which the page location of the data in the affinity clause was determined using \texttt{libnuma}, reducing expensive data movement between NUMA nodes. (2) Temporal mode schedules tasks close to where tasks using the same data were scheduled last potentially improving cache data reuse. When the preferred thread is unavailable, task stealing becomes cache-oblivious and only considers threads within the same NUMA node. 
Our system uses a configurable and user-guided task stealing mechanism where the user configures for each task/taskgroup an acceptable set of CPUs which may steal tasks when the preferred CPUs are not available. 
Our system could be extended to track data usage patterns, instead relying on user-provided affinity constraints, to improve temporal locality.
In~\cite{EvaluationOpenMPTask}, the authors proposed a topology-aware scheduling mechanism with multiple task queues per hardware topology level. Each thread added tasks to its respective topology domain, with topology levels defined as: level 0 (computational node), level 1 (motherboard), level 2 (socket or L3), and level 3 (cores). Their system creates one task list per topology instance, leaving the user to decide which level to use per run. For example, using level 3 provided one queue per core.
They also designed work-stealing mechanisms that were aware of the topology levels, preferring to steal tasks from nearby queues first, potentially improving data locality.
Our design differs by creating one task container per topology instance always, leaving the user to decide on a task/taskgroup basis to which topology instance it will have affinity to.
Our work-stealing mechanism is also topology-aware, leaving the user to decide for which tasks/taskgroups to allow task-stealing and the topology level from which to allow it.

\textbf{\circnum{2} Multi-policy scheduling.}
In~\cite{augonnetStarPUUnifiedPlatform2009b} the authors proposed a multi-policy heterogeneous scheduling system enabling multiple scheduling policies within the same program. Their system supported GPU computation offloading, managing user kernels from the runtime. Each computational resource is modeled as a worker with queue access for pushing and popping tasks.
Queue implementations vary from FIFO, LIFO, to priority queues.
These concepts enable various designs by varying queue numbers and implementations. Our system differs by providing affinity-aware scheduling and allowing users to group tasks for collective scheduling, providing ordering for both individual tasks and taskgroups, and nesting of taskgroups.

\textbf{\circnum{3} Runtime APIs for affinity-scheduling.}
In~\cite{maronasMitigatingNUMAEffect2023} the authors propose a method to mitigate NUMA effects by extending the OmpSs-2 runtime for improved task location. Rather than implementing and using affinity clause information, they use data dependence information. They introduced several components for NUMA-aware scheduling: a tracking system based on a global directory that records NUMA node(s) for each data segment that tracks task dependency data. 
The global directory is used by a new scheduling algorithm that based on data location determines the appropriate NUMA queue assignment. 
The proposed heuristics effectively avoid data movement between NUMA nodes, though they lack extensive study on cache locality effects. 
In~\cite{muddukrishnaTaskSchedulingManycore2013} the authors propose both an API and runtime scheduler. The API allowed data allocation affine to specific CPU L2 caches, while the scheduler used this information to select tasks working on data allocated to the same CPU, minimizing memory access latency.
These approaches provide complementary functionality to our work, where existing runtime systems may determine appropriate numa domains for task execution based on data locality, while our hierarchical scheduler addresses the orthogonal challenge of multi-policy task scheduling with affinity constraints.