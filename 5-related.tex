\section{Related Work}
\label{sec:related}
\todo{In general, compress each paragraph, they may be too verbose}

Several approaches have introduced programming model extensions that allow users to express task data usage, enabling runtimes to make informed scheduling decisions based on data locality. While these extensions require application adaptation to specific programming models, they can provide performance improvements by enabling locality-aware scheduling.

In~\cite{terbovenApproachesTaskAffinity2016}, the authors explore various affinity clause options, each providing different hints to the runtime system regarding task affinity. These hints guide the scheduler in task placement decisions: the affinity clause can act on individual tasks, expressing thread ids where the task should be executed; it can also express data location to which the task is affine, helping the scheduler select a CPU close to the data; it can act on taskgroups to express task distribution policies for tasks in the taskgroup (for example, the \texttt{spread} policy distributes tasks among threads as evenly as possible; the policies considered were \texttt{spread}, \texttt{close}, and \texttt{master} â€” tasks executed by the master thread); and it can act on taskloops, expressing the same task distribution policies mentioned for taskgroups. Our system is similar in that we can configure affinity to groups of tasks or singular tasks. On the other hand, our affinity constraints are configured to specific topological domains (e.g. cores sharing L3) instead of to data or specific thread-ids. In addition to data-locality scheduling, our system allows specifying the scheduling policy for tasks within the same taskgroup, which may be used to improve the parallelism creation patterns since some tasks may liberate more parallelism than others depending on the application details. \todo{Reescribir?}

In subsequent work by the same authors \cite{klinkenbergAssessingTasktoDataAffinity2018a}, they evaluated the affinity clause using their experimental implementation for OpenMP 5.0 in LLVM \libomp.
The affinity clause provides hints to the runtime about task data usage, enabling the scheduler to select appropriate CPUs, potentially those closest to data location. 
Two main modes of action were tested: 1) Domain mode in which the page location of the data in the affinity clause was determined using \texttt{libnuma}. This approach reduces expensive data movement between NUMA nodes. 2) Temporal mode schedules tasks close to where tasks using the same data were scheduled last. This mode potentially improves cache data reuse.  When the preferred thread is unavailable, task stealing becomes cache-oblivious and only considers threads within the same NUMA node. 
Our system similarly uses a task stealing mechanism where the user configures for each task/taskgroup the preferred CPUs, and the acceptable ones, which may steal tasks when the preferred are not available. E.g., tasks can be configured to prefer cores sharing L3 cache, and accept any core in the same NUMA node when no preferred core is available. However, our system does not track data usage patterns, instead relying on user-provided affinity constraints.

In~\cite{EvaluationOpenMPTask}, the authors proposed a topology-aware scheduling mechanism with multiple task queues per hardware topology level. Each thread added tasks to its respective topology domain, with topology levels defined as: level 0 (computational node), level 1 (motherboard), level 2 (socket or L3), and level 3 (cores). Their system creates one task list per topology instance, leaving the user to decide which level to use per run. For example, using level 3 provided one queue per core.
They also designed work-stealing mechanisms that were aware of the topology levels, preferring to steal tasks from nearby queues first, potentially improving data locality.
Our design differs by creating one task container per topology instance always, leaving the user to decide on a task/taskgroup basis to which topology instance it will have affinity to.
Our work-stealing mechanism is also topology-aware, leaving the user to decide for which tasks/taskgroups to allow task-stealing and the topology level from which to allow it. \todo{rewrite for clarity?} 

In~\cite{augonnetStarPUUnifiedPlatform2009b} the authors proposed a multi-policy heterogeneous scheduling system enabling multiple scheduling policies within the same program. Their system supported GPU computation offloading, managing user kernels from the runtime. Each computational resource is modeled as a worker with queue access for pushing and popping tasks.
Queue implementations vary from FIFO, LIFO, to priority queues.
These concepts enable various designs by varying queue numbers and implementations. Our system differs by providing affinity-aware scheduling and allowing users to group tasks for collective scheduling, providing ordering for both individual tasks and taskgroups, and nesting of taskgroups.\todo{Rewrite for clarity?}

In~\cite{maronasMitigatingNUMAEffect2023} the authors propose a method to mitigate NUMA effects by extending the OmpSs-2 runtime for improved task location. Rather than implementing and using affinity clause information, they use data dependence information. They introduced several components for NUMA-aware scheduling: a tracking system based on a global directory that records NUMA node(s) for each data segment that tracks task dependency data. 
The global directory is used by a new scheduling algorithm that based on data location determines the appropriate NUMA queue assignment. 
The proposed heuristics effectively avoid data movement between NUMA nodes, though they lack extensive study on cache locality effects. 
These approaches provide complementary functionality to our work, where existing runtime systems may determine appropriate numa domains for task execution based on data locality, while our hierarchical scheduler addresses the orthogonal challenge of multi-policy task scheduling with affinity constraints and task priority support.