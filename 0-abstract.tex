\begin{abstract}
Fork–join constructs (e.g., OpenMP parallel for static) use fixed work-to-core mappings that enhance cache locality and NUMA affinity but handle load imbalance poorly. In contrast, task-based models rely on dynamic scheduling to distribute ready tasks across idle cores, effectively mitigating imbalance but offering developers only limited, coarse-grained control over task placement—often at the expense of locality and affinity. To address this trade-off, we propose adding flexible, dynamic, and composable scheduling policies to task-based runtimes, enabling developers to preserve the benefits of dynamic load balancing while selectively exploiting locality, affinity, and priority where they matter most.

At large scales, centralized schedulers can become bottlenecks, prompting most runtime systems to prioritize scalability by adopting local queues and work-stealing. This approach sacrifices flexibility, offering only static, basic scheduling policies (e.g., FIFO, LIFO, or Priority) and a few per-task hints such as affinity or priority.
In this paper, we introduce a Hierarchical Scheduling Framework (HSF) that provides fine-grained control and composability of scheduling policies. The core abstraction is the taskgroup: when a task becomes ready, it is inserted into a taskgroup, and taskgroups may contain either individual tasks or nested taskgroups. Each taskgroup can define its own scheduling policy, enabling developers to combine and dynamically adapt multiple policies within a single application. HSF is built on scalable synchronization primitives—such as single-producer/single-consumer queues for task insertion and a delegation lock for efficiently serving idle cores—allowing the implementation of scheduling policies that maintain a global view of the system without sacrificing performance or scalability.

We have implemented several policies—e.g., FIFO, LIFO, Priority—and evaluated them across diverse benchmarks. Results show that composing policies at the taskgroup level guides task execution more effectively, improves performance compared to monolithic work-stealing with static policies, and maintains low scheduler overhead even on large multicore systems. Overall, our design provides a practical path toward application-tailored scheduling in task-based runtimes.
\keywords{Scheduling \and Task-based programming models \and Runtime systems \and HPC \and Affinity \and Locality}
\end{abstract}
